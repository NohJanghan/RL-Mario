{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Google Colab에서 노트북을 실행하실 때에는 \n",
        "# https://tutorials.pytorch.kr/beginner/colab 를 참고하세요.\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "마리오 게임 RL 에이전트로 학습하기\n",
        "==================================\n",
        "\n",
        "**Authors**: [Yuansong Feng](https://github.com/YuansongFeng), [Suraj Subramanian](https://github.com/suraj813), [Howard Wang](https://github.com/hw26), [Steven Guo](https://github.com/GuoYuzhang).\n",
        "\n",
        ":   **번역**: [김태영](https://github.com/Taeyoung96).\n",
        "\n",
        "이번 튜토리얼에서는 심층 강화 학습의 기본 사항들에 대해 이야기해보도록\n",
        "하겠습니다. 마지막에는, 스스로 게임을 할 수 있는 AI 기반 마리오를\n",
        "([Double Deep Q-Networks](https://arxiv.org/pdf/1509.06461.pdf) 사용)\n",
        "구현하게 됩니다.\n",
        "\n",
        "이 튜토리얼에서는 RL에 대한 사전 지식이 필요하지 않지만, 이러한\n",
        "[링크](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html)\n",
        "를 통해 RL 개념에 친숙해 질 수 있으며, 여기 있는\n",
        "[치트시트](https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N)\n",
        "를 활용할 수도 있습니다. 튜토리얼에서 사용하는 전체 코드는\n",
        "[여기](https://github.com/yuansongFeng/MadMario/) 에서 확인 할 수\n",
        "있습니다.\n",
        "\n",
        "![](https://tutorials.pytorch.kr/_static/img/mario.gif)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``` {.sourceCode .bash}\n",
        "%%bash\n",
        "pip install gym-super-mario-bros==7.4.0\n",
        "pip install tensordict==0.3.0\n",
        "pip install torchrl==0.3.0\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision import transforms as T\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from collections import deque\n",
        "import random, datetime, time, os\n",
        "\n",
        "# Gym은 강화학습을 위한 OpenAI 툴킷입니다.\n",
        "import gym\n",
        "from gym.spaces import Box\n",
        "from gym.wrappers import FrameStack\n",
        "\n",
        "# OpenAI Gym을 위한 NES 에뮬레이터\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "\n",
        "# OpenAI Gym에서의 슈퍼 마리오 환경 세팅\n",
        "import gym_super_mario_bros\n",
        "\n",
        "from tensordict import TensorDict\n",
        "from torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "강화학습 개념\n",
        "=============\n",
        "\n",
        "**환경(Environment)** : 에이전트가 상호작용하며 스스로 배우는\n",
        "세계입니다.\n",
        "\n",
        "**행동(Action)** $a$ : 에이전트가 환경에 어떻게 응답하는지 행동을 통해\n",
        "나타냅니다. 가능한 모든 행동의 집합을 *행동 공간* 이라고 합니다.\n",
        "\n",
        "**상태(State)** $s$ : 환경의 현재 특성을 상태를 통해 나타냅니다. 환경이\n",
        "있을 수 있는 모든 가능한 상태 집합을 *상태 공간* 이라고 합니다.\n",
        "\n",
        "**포상(Reward)** $r$ : 포상은 환경에서 에이전트로 전달되는 핵심\n",
        "피드백입니다. 에이전트가 학습하고 향후 행동을 변경하도록 유도하는\n",
        "것입니다. 여러 시간 단계에 걸친 포상의 합을 **리턴(Return)** 이라고\n",
        "합니다.\n",
        "\n",
        "**최적의 행동-가치 함수(Action-Value function)** $Q^*(s,a)$ : 상태 $s$\n",
        "에서 시작하면 예상되는 리턴을 반환하고, 임의의 행동 $a$ 를 선택합니다.\n",
        "그리고 각각의 미래의 단계에서 포상의 합을 극대화하는 행동을 선택하도록\n",
        "합니다. $Q$ 는 상태에서 행동의 \"품질\" 을 나타냅니다. 우리는 이 함수를\n",
        "근사 시키려고 합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "환경(Environment)\n",
        "=================\n",
        "\n",
        "환경 초기화하기\n",
        "---------------\n",
        "\n",
        "마리오 게임에서 환경은 튜브, 버섯, 그 이외 다른 여러 요소들로 구성되어\n",
        "있습니다.\n",
        "\n",
        "마리오가 행동을 취하면, 환경은 변경된 (다음)상태, 포상 그리고 다른\n",
        "정보들로 응답합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### detail setting\n",
        "<br/> SuperMarioBros-\"world\"-\"stage\"-v\"version\"\n",
        "<br>\"world\" is a number in {1, 2, 3, 4, 5, 6, 7, 8} indicating the world\n",
        "<br>\"stage\" is a number in {1, 2, 3, 4} indicating the stage within a world\n",
        "<br>\"version\" is a number in {0, 1, 2, 3} specifying the ROM mode to use\n",
        "\n",
        "<br>0: standard ROM\n",
        "<br>1: downsampled ROM\n",
        "<br>2: pixel ROM\n",
        "<br>3: rectangle ROM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/bagjuhyeon/Documents/WorkSpace/RL-game/.venv/lib/python3.10/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-1-1-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
            "  logger.warn(\n",
            "/Users/bagjuhyeon/Documents/WorkSpace/RL-game/.venv/lib/python3.10/site-packages/gym/envs/registration.py:627: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes', 'video.frames_per_second']\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(240, 256, 3),\n",
            " 0.0,\n",
            " False,\n",
            " {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'y_pos': 79}\n"
          ]
        }
      ],
      "source": [
        "# 슈퍼 마리오 환경 초기화하기 (in v0.26 change render mode to 'human' to see results on the screen)\n",
        "if gym.__version__ < '0.26':\n",
        "    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", new_step_api=True)\n",
        "else:\n",
        "    try:\n",
        "        env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", render_mode='human', apply_api_compatibility=True)\n",
        "    except:\n",
        "        env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", render_mode='rgb-array', apply_api_compatibility=True)\n",
        "\n",
        "# 상태 공간을 3가지로 제한하기\n",
        "#   0. 오른쪽으로 걷기\n",
        "#   1. 오른쪽으로 점프하기\n",
        "#   2. \n",
        "env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"], [\"A\"]])\n",
        "\n",
        "env.reset()\n",
        "next_state, reward, done, trunc, info = env.step(action=0)\n",
        "print(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "환경 전처리 과정 거치기\n",
        "=======================\n",
        "\n",
        "`다음 상태(next_state)` 에서 환경 데이터가 에이전트로 반환됩니다. 앞서\n",
        "살펴보았듯이, 각각의 상태는 `[3, 240, 256]` 의 배열로 나타내고 있습니다.\n",
        "종종 상태가 제공하는 것은 에이전트가 필요로 하는 것보다 더 많은\n",
        "정보입니다. 예를 들어, 마리오의 행동은 파이프의 색깔이나 하늘의 색깔에\n",
        "좌우되지 않습니다!\n",
        "\n",
        "아래에 설명할 클래스들은 환경 데이터를 에이전트에 보내기 전 단계에서\n",
        "전처리 과정에 사용할 **래퍼(Wrappers)** 입니다.\n",
        "\n",
        "`GrayScaleObservation` 은 RGB 이미지를 흑백 이미지로 바꾸는 일반적인\n",
        "래퍼입니다. `GrayScaleObservation` 클래스를 사용하면 유용한 정보를 잃지\n",
        "않고 상태의 크기를 줄일 수 있습니다. `GrayScaleObservation` 를 적용하면\n",
        "각각 상태의 크기는 `[1, 240, 256]` 이 됩니다.\n",
        "\n",
        "`ResizeObservation` 은 각각의 상태(Observation)를 정사각형 이미지로 다운\n",
        "샘플링합니다. 이 래퍼를 적용하면 각각 상태의 크기는 `[1, 84, 84]` 이\n",
        "됩니다.\n",
        "\n",
        "`SkipFrame` 은 `gym.Wrapper` 으로부터 상속을 받은 사용자 지정\n",
        "클래스이고, `step()` 함수를 구현합니다. 왜냐하면 연속되는 프레임은 큰\n",
        "차이가 없기 때문에 n개의 중간 프레임을 큰 정보의 손실 없이 건너뛸 수\n",
        "있기 때문입니다. n번째 프레임은 건너뛴 각 프레임에 걸쳐 누적된 포상을\n",
        "집계합니다.\n",
        "\n",
        "`FrameStack` 은 환경의 연속 프레임을 단일 관찰 지점으로 바꾸어 학습\n",
        "모델에 제공할 수 있는 래퍼입니다. 이렇게 하면 마리오가 착지 중이였는지\n",
        "또는 점프 중이었는지 이전 몇 프레임의 움직임 방향에 따라 확인할 수\n",
        "있습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class SkipFrame(gym.Wrapper):\n",
        "    def __init__(self, env, skip):\n",
        "        \"\"\"모든 `skip` 프레임만 반환합니다.\"\"\"\n",
        "        super().__init__(env)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"행동을 반복하고 포상을 더합니다.\"\"\"\n",
        "        total_reward = 0.0\n",
        "        for i in range(self._skip):\n",
        "            # 포상을 누적하고 동일한 작업을 반복합니다.\n",
        "            obs, reward, done, trunk, info = self.env.step(action)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        return obs, total_reward, done, trunk, info\n",
        "\n",
        "\n",
        "class GrayScaleObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        obs_shape = self.observation_space.shape[:2]\n",
        "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "    def permute_orientation(self, observation):\n",
        "        # [H, W, C] 배열을 [C, H, W] 텐서로 바꿉니다.\n",
        "        observation = np.transpose(observation, (2, 0, 1))\n",
        "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
        "        return observation\n",
        "\n",
        "    def observation(self, observation):\n",
        "        observation = self.permute_orientation(observation)\n",
        "        transform = T.Grayscale()\n",
        "        observation = transform(observation)\n",
        "        return observation\n",
        "\n",
        "\n",
        "class ResizeObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env, shape):\n",
        "        super().__init__(env)\n",
        "        if isinstance(shape, int):\n",
        "            self.shape = (shape, shape)\n",
        "        else:\n",
        "            self.shape = tuple(shape)\n",
        "\n",
        "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
        "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        transforms = T.Compose(\n",
        "            [T.Resize(self.shape, antialias=True), T.Normalize(0, 255)]\n",
        "        )\n",
        "        observation = transforms(observation).squeeze(0)\n",
        "        return observation\n",
        "\n",
        "\n",
        "# 래퍼를 환경에 적용합니다.\n",
        "env = SkipFrame(env, skip=4)\n",
        "env = GrayScaleObservation(env)\n",
        "env = ResizeObservation(env, shape=84)\n",
        "if gym.__version__ < '0.26':\n",
        "    env = FrameStack(env, num_stack=4, new_step_api=True)\n",
        "else:\n",
        "    env = FrameStack(env, num_stack=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "앞서 소개한 래퍼를 환경에 적용한 후, 최종 래핑 상태는 왼쪽 아래 이미지에\n",
        "표시된 것처럼 4개의 연속된 흑백 프레임으로 구성됩니다. 마리오가 행동을\n",
        "할 때마다, 환경은 이 구조의 상태로 응답합니다. 구조는 `[4, 84, 84]`\n",
        "크기의 3차원 배열로 구성되어 있습니다.\n",
        "\n",
        "![](https://tutorials.pytorch.kr/_static/img/mario_env.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "에이전트(Agent)\n",
        "===============\n",
        "\n",
        "`Mario` 라는 클래스를 이 게임의 에이전트로 생성합니다. 마리오는 다음과\n",
        "같은 기능을 할 수 있어야 합니다.\n",
        "\n",
        "-   **행동(Act)** 은 (환경의) 현재 상태를 기반으로 최적의 행동 정책에\n",
        "    따라 정해집니다.\n",
        "-   경험을 **기억(Remember)** 하는 것. 경험은 (현재 상태, 현재 행동,\n",
        "    포상, 다음 상태) 로 이루어져 있습니다. 마리오는 그의 행동 정책을\n",
        "    업데이트 하기 위해 *캐시(caches)* 를 한 다음, 그의 경험을\n",
        "    *리콜(recalls)* 합니다.\n",
        "-   **학습(Learn)** 을 통해 시간이 지남에 따라 더 나은 행동 정책을\n",
        "    택합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Mario:\n",
        "    def __init__():\n",
        "        pass\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"상태가 주어지면, 입실론-그리디 행동(epsilon-greedy action)을 선택해야 합니다.\"\"\"\n",
        "        pass\n",
        "\n",
        "    def cache(self, experience):\n",
        "        \"\"\"메모리에 경험을 추가합니다.\"\"\"\n",
        "        pass\n",
        "\n",
        "    def recall(self):\n",
        "        \"\"\"메모리로부터 경험을 샘플링합니다.\"\"\"\n",
        "        pass\n",
        "\n",
        "    def learn(self):\n",
        "        \"\"\"일련의 경험들로 실시간 행동 가치(online action value) (Q) 함수를 업데이트 합니다.\"\"\"\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "이번 섹션에서는 마리오 클래스의 매개변수를 채우고, 마리오 클래스의\n",
        "함수들을 정의하겠습니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "행동하기(Act)\n",
        "=============\n",
        "\n",
        "주어진 상태에 대해, 에이전트는 최적의 행동을 이용할 것인지 임의의 행동을\n",
        "선택하여 분석할 것인지 선택할 수 있습니다.\n",
        "\n",
        "마리오는 임의의 행동을 선택했을 때 `self.exploration_rate` 를\n",
        "활용합니다. 최적의 행동을 이용한다고 했을 때, 그는 최적의 행동을\n",
        "수행하기 위해 (`학습하기(Learn)` 섹션에서 구현된) `MarioNet` 이\n",
        "필요합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Mario:\n",
        "    def __init__(self, state_dim, action_dim, save_dir):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.save_dir = save_dir\n",
        "\n",
        "        if torch.backends.mps.is_available():\n",
        "            self.device = \"mps\"\n",
        "        elif torch.cuda.is_available():\n",
        "            self.device = \"cuda\"\n",
        "        else:\n",
        "            self.device = \"cpu\"\n",
        "\n",
        "        # 마리오의 DNN은 최적의 행동을 예측합니다 - 이는 학습하기 섹션에서 구현합니다.\n",
        "        self.net = MarioNet(self.state_dim, self.action_dim).float()\n",
        "        self.net = self.net.to(device=self.device)\n",
        "\n",
        "        self.exploration_rate = 1\n",
        "        #self.exploration_rate_decay = 0.99999975\n",
        "        self.exploration_rate_decay = 0.99\n",
        "        self.exploration_rate_min = 0.1\n",
        "        self.curr_step = 0\n",
        "\n",
        "        self.save_every = 5e4  # Mario Net 저장 사이의 경험 횟수\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"\n",
        "    주어진 상태에서, 입실론-그리디 행동(epsilon-greedy action)을 선택하고, 스텝의 값을 업데이트 합니다.\n",
        "\n",
        "    입력값:\n",
        "    state (``LazyFrame``): 현재 상태에서의 단일 상태(observation)값을 말합니다. 차원은 (state_dim)입니다.\n",
        "    출력값:\n",
        "    ``action_idx`` (int): Mario가 수행할 행동을 나타내는 정수 값입니다.\n",
        "    \"\"\"\n",
        "        # 임의의 행동을 선택하기\n",
        "        if np.random.rand() < self.exploration_rate:\n",
        "            action_idx = np.random.randint(self.action_dim)\n",
        "\n",
        "        # 최적의 행동을 이용하기\n",
        "        else:\n",
        "            state = state[0].__array__() if isinstance(state, tuple) else state.__array__()\n",
        "            state = torch.tensor(state, device=self.device).unsqueeze(0)\n",
        "            action_values = self.net(state, model=\"online\")\n",
        "            action_idx = torch.argmax(action_values, axis=1).item()\n",
        "\n",
        "        # exploration_rate 감소하기\n",
        "        self.exploration_rate *= self.exploration_rate_decay\n",
        "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
        "\n",
        "        # 스텝 수 증가하기\n",
        "        self.curr_step += 1\n",
        "        return action_idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "캐시(Cache)와 리콜(Recall)하기\n",
        "==============================\n",
        "\n",
        "이 두가지 함수는 마리오의 \"메모리\" 프로세스 역할을 합니다.\n",
        "\n",
        "`cache()`: 마리오가 행동을 할 때마다, 그는 `경험` 을 그의 메모리에\n",
        "저장합니다. 그의 경험에는 현재 *상태* 에 따른 수행된 *행동* ,\n",
        "행동으로부터 얻은 *포상* , *다음 상태*, 그리고 게임 *완료* 여부가\n",
        "포함됩니다.\n",
        "\n",
        "`recall()`: Mario는 자신의 기억에서 무작위로 일련의 경험을 샘플링하여\n",
        "게임을 학습하는 데 사용합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Mario(Mario):  # 연속성을 위한 하위 클래스입니다.\n",
        "    def __init__(self, state_dim, action_dim, save_dir):\n",
        "        super().__init__(state_dim, action_dim, save_dir)\n",
        "        self.memory = TensorDictReplayBuffer(storage=LazyMemmapStorage(100000, device=torch.device(\"cpu\")))\n",
        "        self.batch_size = 128\n",
        "\n",
        "    def cache(self, state, next_state, action, reward, done):\n",
        "        \"\"\"\n",
        "        Store the experience to self.memory (replay buffer)\n",
        "\n",
        "        입력값:\n",
        "        state (``LazyFrame``),\n",
        "        next_state (``LazyFrame``),\n",
        "        action (``int``),\n",
        "        reward (``float``),\n",
        "        done(``bool``))\n",
        "        \"\"\"\n",
        "        def first_if_tuple(x):\n",
        "            return x[0] if isinstance(x, tuple) else x\n",
        "        state = first_if_tuple(state).__array__()\n",
        "        next_state = first_if_tuple(next_state).__array__()\n",
        "\n",
        "        state = torch.tensor(state)\n",
        "        next_state = torch.tensor(next_state)\n",
        "        action = torch.tensor([action])\n",
        "        reward = torch.tensor([reward])\n",
        "        done = torch.tensor([done])\n",
        "\n",
        "        # self.memory.append((state, next_state, action, reward, done,))\n",
        "        self.memory.add(TensorDict({\"state\": state, \"next_state\": next_state, \"action\": action, \"reward\": reward, \"done\": done}, batch_size=[]))\n",
        "\n",
        "    def recall(self):\n",
        "        \"\"\"\n",
        "        메모리에서 일련의 경험들을 검색합니다.\n",
        "        \"\"\"\n",
        "        batch = self.memory.sample(self.batch_size).to(self.device)\n",
        "        state, next_state, action, reward, done = (batch.get(key) for key in (\"state\", \"next_state\", \"action\", \"reward\", \"done\"))\n",
        "        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "학습하기(Learn)\n",
        "===============\n",
        "\n",
        "마리오는 [DDQN 알고리즘](https://arxiv.org/pdf/1509.06461) 을\n",
        "사용합니다. DDQN 두개의 ConvNets ( $Q_{online}$ 과 $Q_{target}$ ) 을\n",
        "사용하고, 독립적으로 최적의 행동-가치 함수에 근사시키려고 합니다.\n",
        "\n",
        "구현을 할 때, 특징 생성기에서 `특징들` 을 $Q_{online}$ 와 $Q_{target}$\n",
        "에 공유합니다. 그러나 각각의 FC 분류기는 가지고 있도록 설계합니다.\n",
        "$\\theta_{target}$ ($Q_{target}$ 의 매개변수 값) 는 역전파에 의해 값이\n",
        "업데이트 되지 않도록 고정되었습니다. 대신, $\\theta_{online}$ 와\n",
        "주기적으로 동기화를 진행합니다. 이것에 대해서는 추후에 다루도록\n",
        "하겠습니다.)\n",
        "\n",
        "신경망\n",
        "------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class MarioNet(nn.Module):\n",
        "    \"\"\"작은 CNN 구조\n",
        "  입력 -> (conv2d + relu) x 3 -> flatten -> (dense + relu) x 2 -> 출력\n",
        "  \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        c, h, w = input_dim\n",
        "\n",
        "        if h != 84:\n",
        "            raise ValueError(f\"Expecting input height: 84, got: {h}\")\n",
        "        if w != 84:\n",
        "            raise ValueError(f\"Expecting input width: 84, got: {w}\")\n",
        "\n",
        "        self.online = self.__build_cnn(c, output_dim)\n",
        "\n",
        "        self.target = self.__build_cnn(c, output_dim)\n",
        "        self.target.load_state_dict(self.online.state_dict())\n",
        "\n",
        "        # Q_target 매개변수 값은 고정시킵니다.\n",
        "        for p in self.target.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    def forward(self, input, model):\n",
        "        if model == \"online\":\n",
        "            return self.online(input)\n",
        "        elif model == \"target\":\n",
        "            return self.target(input)\n",
        "\n",
        "    def __build_cnn(self, c, output_dim):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(3136, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, output_dim),\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TD 추정 & TD 목표값\n",
        "===================\n",
        "\n",
        "학습을 하는데 두 가지 값들이 포함됩니다.\n",
        "\n",
        "**TD 추정** - 주어진 상태 $s$ 에서 최적의 예측 $Q^*$.\n",
        "\n",
        "$${TD}_e = Q_{online}^*(s,a)$$\n",
        "\n",
        "**TD 목표** - 현재의 포상과 다음상태 $s'$ 에서 추정된 $Q^*$ 의 합.\n",
        "\n",
        "$$a' = argmax_{a} Q_{online}(s', a)$$\n",
        "\n",
        "$${TD}_t = r + \\gamma Q_{target}^*(s',a')$$\n",
        "\n",
        "다음 행동 $a'$ 가 어떨지 모르기 때문에 다음 상태 $s'$ 에서 $Q_{online}$\n",
        "값이 최대가 되도록 하는 행동 $a'$ 를 사용합니다.\n",
        "\n",
        "여기에서 변화도 계산을 비활성화하기 위해 `td_target()` 에서\n",
        "[\\@torch.no\\_grad()](https://pytorch.org/docs/stable/generated/torch.no_grad.html#no-grad)\n",
        "데코레이터(decorator)를 사용합니다. ($\\theta_{target}$ 의 역전파 계산이\n",
        "필요로 하지 않기 때문입니다.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Mario(Mario):\n",
        "    def __init__(self, state_dim, action_dim, save_dir):\n",
        "        super().__init__(state_dim, action_dim, save_dir)\n",
        "        # discount_factor <- greedy 하게 해도 충분히 좋지 않을까? 당장 안 죽는게 더 중요하잖아.\n",
        "        self.gamma = 0.7\n",
        "\n",
        "    def td_estimate(self, state, action):\n",
        "        current_Q = self.net(state, model=\"online\")[\n",
        "            np.arange(0, self.batch_size), action\n",
        "        ]  # Q_online(s,a)\n",
        "        return current_Q\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def td_target(self, reward, next_state, done):\n",
        "        next_state_Q = self.net(next_state, model=\"online\")\n",
        "        best_action = torch.argmax(next_state_Q, axis=1)\n",
        "        next_Q = self.net(next_state, model=\"target\")[\n",
        "            np.arange(0, self.batch_size), best_action\n",
        "        ]\n",
        "        return (reward + (1 - done.float()) * self.gamma * next_Q).float()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "모델 업데이트\n",
        "=============\n",
        "\n",
        "마리오가 재생 버퍼에서 입력을 샘플링할 때, $TD_t$ 와 $TD_e$ 를\n",
        "계산합니다. 그리고 이 손실을 이용하여 $Q_{online}$ 역전파하여 매개변수\n",
        "$\\theta_{online}$ 를 업데이트합니다. ($\\alpha$ 는 `optimizer` 에\n",
        "전달되는 학습률 `lr` 입니다.)\n",
        "\n",
        "$$\\theta_{online} \\leftarrow \\theta_{online} + \\alpha \\nabla(TD_e - TD_t)$$\n",
        "\n",
        "$\\theta_{target}$ 은 역전파를 통해 업데이트 되지 않습니다. 대신,\n",
        "주기적으로 $\\theta_{online}$ 의 값을 $\\theta_{target}$ 로 복사합니다.\n",
        "\n",
        "$$\\theta_{target} \\leftarrow \\theta_{online}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Mario(Mario):\n",
        "    def __init__(self, state_dim, action_dim, save_dir):\n",
        "        super().__init__(state_dim, action_dim, save_dir)\n",
        "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n",
        "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
        "\n",
        "    def update_Q_online(self, td_estimate, td_target):\n",
        "        loss = self.loss_fn(td_estimate, td_target)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss.item()\n",
        "\n",
        "    def sync_Q_target(self):\n",
        "        self.net.target.load_state_dict(self.net.online.state_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "체크포인트 저장\n",
        "===============\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Mario(Mario):\n",
        "    def save(self):\n",
        "        save_path = (\n",
        "            self.save_dir / f\"mario_net_{int(self.curr_step // self.save_every)}.chkpt\"\n",
        "        )\n",
        "        torch.save(\n",
        "            dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate),\n",
        "            save_path,\n",
        "        )\n",
        "        print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "모든 기능을 합치기\n",
        "==================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Mario(Mario):\n",
        "    def __init__(self, state_dim, action_dim, save_dir):\n",
        "        super().__init__(state_dim, action_dim, save_dir)\n",
        "        self.burnin = 1e4  # 학습을 진행하기 전 최소한의 경험값.\n",
        "        self.learn_every = 3  # Q_online 업데이트 사이의 경험 횟수.\n",
        "        self.sync_every = 1e4  # Q_target과 Q_online sync 사이의 경험 수\n",
        "\n",
        "    def learn(self):\n",
        "        if self.curr_step % self.sync_every == 0:\n",
        "            self.sync_Q_target()\n",
        "\n",
        "        if self.curr_step % self.save_every == 0:\n",
        "            self.save()\n",
        "\n",
        "        if self.curr_step < self.burnin:\n",
        "            return None, None\n",
        "\n",
        "        if self.curr_step % self.learn_every != 0:\n",
        "            return None, None\n",
        "\n",
        "        # 메모리로부터 샘플링을 합니다.\n",
        "        state, next_state, action, reward, done = self.recall()\n",
        "\n",
        "        # TD 추정값을 가져옵니다.\n",
        "        td_est = self.td_estimate(state, action)\n",
        "\n",
        "        # TD 목표값을 가져옵니다.\n",
        "        td_tgt = self.td_target(reward, next_state, done)\n",
        "\n",
        "        # 실시간 Q(Q_online)을 통해 역전파 손실을 계산합니다.\n",
        "        loss = self.update_Q_online(td_est, td_tgt)\n",
        "\n",
        "        return (td_est.mean().item(), loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "기록하기\n",
        "========\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time, datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class MetricLogger:\n",
        "    def __init__(self, save_dir):\n",
        "        self.save_log = save_dir / \"log\"\n",
        "        with open(self.save_log, \"w\") as f:\n",
        "            f.write(\n",
        "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
        "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
        "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
        "            )\n",
        "\n",
        "         # 텐서보드 writer 초기화\n",
        "        self.writer = SummaryWriter(log_dir=str(save_dir / \"tensorboard\"))\n",
        "        \n",
        "        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n",
        "        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n",
        "        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n",
        "        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n",
        "\n",
        "        # 지표(Metric)와 관련된 리스트입니다.\n",
        "        self.ep_rewards = []\n",
        "        self.ep_lengths = []\n",
        "        self.ep_avg_losses = []\n",
        "        self.ep_avg_qs = []\n",
        "\n",
        "        # 모든 record() 함수를 호출한 후 이동 평균(Moving average)을 계산합니다.\n",
        "        self.moving_avg_ep_rewards = []\n",
        "        self.moving_avg_ep_lengths = []\n",
        "        self.moving_avg_ep_avg_losses = []\n",
        "        self.moving_avg_ep_avg_qs = []\n",
        "\n",
        "        # 현재 에피스드에 대한 지표를 기록합니다.\n",
        "        self.init_episode()\n",
        "\n",
        "        # 시간에 대한 기록입니다.\n",
        "        self.record_time = time.time()\n",
        "\n",
        "    def log_step(self, reward, loss, q):\n",
        "        self.curr_ep_reward += reward\n",
        "        self.curr_ep_length += 1\n",
        "        if loss:\n",
        "            self.curr_ep_loss += loss\n",
        "            self.curr_ep_q += q\n",
        "            self.curr_ep_loss_length += 1\n",
        "\n",
        "    def log_episode(self):\n",
        "        \"에피스드의 끝을 표시합니다.\"\n",
        "        self.ep_rewards.append(self.curr_ep_reward)\n",
        "        self.ep_lengths.append(self.curr_ep_length)\n",
        "        if self.curr_ep_loss_length == 0:\n",
        "            ep_avg_loss = 0\n",
        "            ep_avg_q = 0\n",
        "        else:\n",
        "            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
        "            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
        "        self.ep_avg_losses.append(ep_avg_loss)\n",
        "        self.ep_avg_qs.append(ep_avg_q)\n",
        "\n",
        "        self.init_episode()\n",
        "\n",
        "    def init_episode(self):\n",
        "        self.curr_ep_reward = 0.0\n",
        "        self.curr_ep_length = 0\n",
        "        self.curr_ep_loss = 0.0\n",
        "        self.curr_ep_q = 0.0\n",
        "        self.curr_ep_loss_length = 0\n",
        "\n",
        "    def record(self, episode, epsilon, step):\n",
        "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
        "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
        "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
        "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
        "\n",
        "        # 텐서보드에 기록\n",
        "        self.writer.add_scalar('Metrics/Mean Reward', mean_ep_reward, episode)\n",
        "        self.writer.add_scalar('Metrics/Mean Length', mean_ep_length, episode)\n",
        "        self.writer.add_scalar('Metrics/Mean Loss', mean_ep_loss, episode)\n",
        "        self.writer.add_scalar('Metrics/Mean Q Value', mean_ep_q, episode)\n",
        "        self.writer.add_scalar('Metrics/Epsilon', epsilon, episode)\n",
        "        \n",
        "        # 이동 평균\n",
        "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
        "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
        "        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
        "        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
        "\n",
        "        last_record_time = self.record_time\n",
        "        self.record_time = time.time()\n",
        "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
        "\n",
        "        print(\n",
        "            f\"Episode {episode} - \"\n",
        "            f\"Step {step} - \"\n",
        "            f\"Epsilon {epsilon} - \"\n",
        "            f\"Mean Reward {mean_ep_reward} - \"\n",
        "            f\"Mean Length {mean_ep_length} - \"\n",
        "            f\"Mean Loss {mean_ep_loss} - \"\n",
        "            f\"Mean Q Value {mean_ep_q} - \"\n",
        "            f\"Time Delta {time_since_last_record} - \"\n",
        "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
        "        )\n",
        "\n",
        "        with open(self.save_log, \"a\") as f:\n",
        "            f.write(\n",
        "                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
        "                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n",
        "                f\"{time_since_last_record:15.3f}\"\n",
        "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
        "            )\n",
        "\n",
        "        for metric in [\"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\", \"ep_rewards\"]:\n",
        "            plt.clf()\n",
        "            plt.plot(getattr(self, f\"moving_avg_{metric}\"), label=f\"moving_avg_{metric}\")\n",
        "            plt.legend()\n",
        "            plt.savefig(getattr(self, f\"{metric}_plot\"))\n",
        "    def close(self):\n",
        "        self.writer.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "게임을 실행시켜봅시다!\n",
        "======================\n",
        "\n",
        "이번 예제에서는 40개의 에피소드에 대해 학습 루프를 실행시켰습니다.하지만\n",
        "마리오가 진정으로 세계를 학습하기 위해서는 적어도 40000개의 에피소드에\n",
        "대해 학습을 시킬 것을 제안합니다!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using CUDA: False\n",
            "\n",
            "Episode 0 - Step 113 - Epsilon 0.3212010745647914 - Mean Reward 614.0 - Mean Length 113.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 2.033 - Time 2025-05-12T21:02:42\n",
            "Episode 20 - Step 11816 - Epsilon 0.1 - Mean Reward 424.19 - Mean Length 562.667 - Mean Loss 0.104 - Mean Q Value 0.402 - Time Delta 226.283 - Time 2025-05-12T21:06:29\n",
            "Episode 40 - Step 15942 - Epsilon 0.1 - Mean Reward 607.683 - Mean Length 388.829 - Mean Loss 0.134 - Mean Q Value 0.851 - Time Delta 110.379 - Time 2025-05-12T21:08:19\n",
            "Episode 60 - Step 19053 - Epsilon 0.1 - Mean Reward 637.098 - Mean Length 312.344 - Mean Loss 0.145 - Mean Q Value 1.193 - Time Delta 83.72 - Time 2025-05-12T21:09:43\n",
            "Episode 80 - Step 21715 - Epsilon 0.1 - Mean Reward 644.321 - Mean Length 268.086 - Mean Loss 0.163 - Mean Q Value 1.723 - Time Delta 70.457 - Time 2025-05-12T21:10:53\n",
            "Episode 100 - Step 25110 - Epsilon 0.1 - Mean Reward 669.75 - Mean Length 249.97 - Mean Loss 0.171 - Mean Q Value 2.277 - Time Delta 91.053 - Time 2025-05-12T21:12:24\n",
            "Episode 120 - Step 29811 - Epsilon 0.1 - Mean Reward 753.19 - Mean Length 179.95 - Mean Loss 0.185 - Mean Q Value 3.165 - Time Delta 126.982 - Time 2025-05-12T21:14:31\n",
            "Episode 140 - Step 33724 - Epsilon 0.1 - Mean Reward 755.03 - Mean Length 177.82 - Mean Loss 0.198 - Mean Q Value 4.215 - Time Delta 109.021 - Time 2025-05-12T21:16:20\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[13], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m next_state, reward, done, trunc, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# 기억하기\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[43mmario\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# 배우기\u001b[39;00m\n\u001b[1;32m     30\u001b[0m q, loss \u001b[38;5;241m=\u001b[39m mario\u001b[38;5;241m.\u001b[39mlearn()\n",
            "Cell \u001b[0;32mIn[6], line 23\u001b[0m, in \u001b[0;36mMario.cache\u001b[0;34m(self, state, next_state, action, reward, done)\u001b[0m\n\u001b[1;32m     20\u001b[0m state \u001b[38;5;241m=\u001b[39m first_if_tuple(state)\u001b[38;5;241m.\u001b[39m__array__()\n\u001b[1;32m     21\u001b[0m next_state \u001b[38;5;241m=\u001b[39m first_if_tuple(next_state)\u001b[38;5;241m.\u001b[39m__array__()\n\u001b[0;32m---> 23\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m next_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(next_state)\n\u001b[1;32m     25\u001b[0m action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([action])\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "print(f\"Using CUDA: {use_cuda}\")\n",
        "print()\n",
        "\n",
        "save_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
        "save_dir.mkdir(parents=True)\n",
        "\n",
        "mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)\n",
        "\n",
        "logger = MetricLogger(save_dir)\n",
        "\n",
        "episodes = 40000\n",
        "for e in range(episodes):\n",
        "\n",
        "    state = env.reset()\n",
        "\n",
        "    # 게임을 실행시켜봅시다!\n",
        "    while True:\n",
        "\n",
        "        # 현재 상태에서 에이전트 실행하기\n",
        "        action = mario.act(state)\n",
        "\n",
        "        # 에이전트가 액션 수행하기\n",
        "        next_state, reward, done, trunc, info = env.step(action)\n",
        "\n",
        "        # 기억하기\n",
        "        mario.cache(state, next_state, action, reward, done)\n",
        "\n",
        "        # 배우기\n",
        "        q, loss = mario.learn()\n",
        "\n",
        "        # 기록하기\n",
        "        logger.log_step(reward, loss, q)\n",
        "\n",
        "        # 상태 업데이트하기\n",
        "        state = next_state\n",
        "\n",
        "        # 게임이 끝났는지 확인하기\n",
        "        if done or info[\"flag_get\"]:\n",
        "            break\n",
        "\n",
        "    logger.log_episode()\n",
        "\n",
        "    if (e % 20 == 0) or (e == episodes - 1):\n",
        "        logger.record(episode=e, epsilon=mario.exploration_rate, step=mario.curr_step)\n",
        "logger.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.multiprocessing as mp\n",
        "from torch.multiprocessing import Process, Queue\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "class ParallelEnv:\n",
        "    def __init__(self, num_envs=4):\n",
        "        self.num_envs = num_envs\n",
        "        self.envs = []\n",
        "        self.queues = []\n",
        "        \n",
        "        for _ in range(num_envs):\n",
        "            env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", render_mode='rgb-array', apply_api_compatibility=True)\n",
        "            env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n",
        "            env = SkipFrame(env, skip=4)\n",
        "            env = GrayScaleObservation(env)\n",
        "            env = ResizeObservation(env, shape=84)\n",
        "            env = FrameStack(env, num_stack=4)\n",
        "            self.envs.append(env)\n",
        "            self.queues.append(Queue())\n",
        "    \n",
        "    def reset(self):\n",
        "        states = []\n",
        "        for env in self.envs:\n",
        "            state = env.reset()\n",
        "            states.append(state)\n",
        "        return states\n",
        "    \n",
        "    def step(self, actions):\n",
        "        states = []\n",
        "        rewards = []\n",
        "        dones = []\n",
        "        infos = []\n",
        "        \n",
        "        for i, (env, action) in enumerate(zip(self.envs, actions)):\n",
        "            state, reward, done, trunc, info = env.step(action)\n",
        "            states.append(state)\n",
        "            rewards.append(reward)\n",
        "            dones.append(done)\n",
        "            infos.append(info)\n",
        "            \n",
        "            if done:\n",
        "                state = env.reset()\n",
        "                states[i] = state\n",
        "        \n",
        "        return states, rewards, dones, infos\n",
        "\n",
        "def train_worker(worker_id, shared_memory, num_episodes):\n",
        "    # 각 워커별 환경 초기화\n",
        "    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", render_mode='rgb-array', apply_api_compatibility=True)\n",
        "    env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n",
        "    env = SkipFrame(env, skip=4)\n",
        "    env = GrayScaleObservation(env)\n",
        "    env = ResizeObservation(env, shape=84)\n",
        "    env = FrameStack(env, num_stack=4)\n",
        "    \n",
        "    # 에이전트 초기화\n",
        "    mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=Path(\"checkpoints\"))\n",
        "    \n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        episode_reward = 0\n",
        "        \n",
        "        while True:\n",
        "            action = mario.act(state)\n",
        "            next_state, reward, done, trunc, info = env.step(action)\n",
        "            \n",
        "            # 경험 저장\n",
        "            mario.cache(state, next_state, action, reward, done)\n",
        "            \n",
        "            # 학습\n",
        "            q, loss = mario.learn()\n",
        "            \n",
        "            episode_reward += reward\n",
        "            state = next_state\n",
        "            \n",
        "            if done or info[\"flag_get\"]:\n",
        "                break\n",
        "        \n",
        "        # 공유 메모리에 결과 저장\n",
        "        shared_memory.put({\n",
        "            'worker_id': worker_id,\n",
        "            'episode': episode,\n",
        "            'reward': episode_reward\n",
        "        })\n",
        "\n",
        "def main():\n",
        "    num_workers = 4  # 병렬로 실행할 워커 수\n",
        "    num_episodes = 10000  # 각 워커당 에피소드 수\n",
        "    \n",
        "    # 공유 메모리 초기화\n",
        "    shared_memory = Queue()\n",
        "    \n",
        "    # 워커 프로세스 생성\n",
        "    processes = []\n",
        "    for i in range(num_workers):\n",
        "        p = Process(target=train_worker, args=(i, shared_memory, num_episodes))\n",
        "        p.start()\n",
        "        processes.append(p)\n",
        "    \n",
        "    # 메인 에이전트 초기화\n",
        "    mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=Path(\"checkpoints\"))\n",
        "    logger = MetricLogger(save_dir=Path(\"checkpoints\"))\n",
        "    \n",
        "    # 결과 수집 및 로깅\n",
        "    total_episodes = 0\n",
        "    while total_episodes < num_workers * num_episodes:\n",
        "        result = shared_memory.get()\n",
        "        logger.log_episode()\n",
        "        logger.record(\n",
        "            episode=result['episode'],\n",
        "            epsilon=mario.exploration_rate,\n",
        "            step=mario.curr_step\n",
        "        )\n",
        "        total_episodes += 1\n",
        "    \n",
        "    # 프로세스 종료\n",
        "    for p in processes:\n",
        "        p.join()\n",
        "    \n",
        "    logger.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    mp.set_start_method('spawn')  # Windows 호환성을 위해 필요\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "결론\n",
        "====\n",
        "\n",
        "이 튜토리얼에서는 PyTorch를 사용하여 게임 플레이 AI를 훈련하는 방법을\n",
        "살펴보았습니다. [OpenAI gym](https://gym.openai.com/) 에 있는 어떤\n",
        "게임이든 동일한 방법으로 AI를 훈련시키고 게임을 진행할 수 있습니다. 이\n",
        "튜토리얼이 도움이 되었기를 바라며, [Github\n",
        "저장소](https://github.com/yuansongFeng/MadMario/) 에서 편하게\n",
        "저자들에게 연락을 하셔도 됩니다!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
